# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

## 研究成果

一个神经网络，以同一个物体/场景在不同视角下的图像及其对应视角作为数据集进行训练，给定一个全新的视角，生成这个物体/场景在这个全新视角下的新的图像. 

## 实现方式

![NeRF Structure](/Assets/Images/NeRF%20structure.png)

需要训练的网络只有(a)到(b)的这一部分，这一部分的网络用于估计在给定的视角和位置下，一点的密度和色彩. 

在(c)中，本文从摄像机的视角发射数条光线，在光线的路径上对最终呈现的色彩进行积分，积分采用这样一个公式：

![NeRF Formula 1](/Assets/Images/NeRF%20Formula%201.png)

$\sigma_j$和$\delta_j$分别表示第$j$个采样点的密度和到上一个采样点的间隔，从而$T_i$就表示从光线源点（屏幕）到第$i$个采样点的光线衰减. 提出这个系数的目的是为了让离屏幕更近的点对色彩的影响更大，同时让光线在遇到第一个具有密度的采样点之前都不衰减. $(1-e^{-\sigma_i\delta_i})$这一项把一点对色彩的影响映射到[0,1)这个区间. $\mathbb{c_i}$是该采样点的色彩. 

(d)表示这个网络的损失函数就是图像上每条光线最终呈现的色彩与真实色彩之差的2-范数之和，神经网络训练的后向传播阶段将这个损失经过可微的(c)传回神经网络并修正网络的权重. 

## 网络优化

### 位置编码

如果仅仅以三维空间中的坐标作为输入，容易对低频色彩的输入产生过拟合，所以本文将每个空间维度上的坐标分解成多个不同频率的编码，再将其作为神经网络的输入：

![NeRF Formula 2](/Assets/Images/NeRF%20Formula%202.png)

### 分级采样

每次都在光线路径上完整采样效率低下，所以本文将采样过程分成了粗采样和精采样两个阶段，粗采样在光线路径上均匀采样但设置较大的间隔，随后再根据粗采样的结果选定精采样区间，以略过空白和被遮挡的空间. 最后的损失函数也相应变为粗采样得到的色彩与精采样得到的色彩之和. 

# Point-NeRF: Point-based Neural Radiance Fields

## 研究成果

在NeRF的基础上加入了点云作为模型，并显式表示点在物体表面的置信度，以此达到比NeRF更快更好的效果. 

## 核心思想

![Point-NeRF Structure](/Assets/Images/Point-NeRF%20structure.png)

本文与NeRF具有相同的目的，即以同一个物体/场景在不同视角下的图像及其对应视角作为数据集，生成这个物体/场景在全新的视角下的新的图像. 

相较于NeRF原文，Point-NeRF主要进行了以下改动：
- 不再使用一个MLP隐式表示物体的表面和色彩，而是用一个每点带有特征向量和置信度的点云来隐式表示. 

    - 这个点云的点的位置及置信度是向MVSNet输入多视角的图像训练集获得的，在这个过程中会求得每张图像的深度图. 一个点的置信度表示这个点在物体边界上的概率大小，在用于后文的加权求和时，每个点的贡献都会乘以这个置信度，用来减少物体内部点对于渲染结果的影响. 

    - 这个点云的点的特征向量则是由一个CNN在图像的基础上依像素求得，随后本文结合深度图将这个特征向量反向投影到点上. 

- 沿光线路径积分时，对于每个采样位置，取其周围最近的$K$个点，将这些点的特征向量以该点距离采样位置距离的反比为权重加权求和，得到该位置的特征向量，用该特征向量和视角求得该点的色彩. 

其余的积分细节与NeRF基本相同，包含密度、衰减等. 

## 网络架构

![Point-NeRF Updating](/Assets/Images/Point-NeRF%20updating.png)

$G_{p,\gamma}(\cdot)$：一个MVSNet，输入一组同一个物体/场景在多视角下的图像，输出对应的点云和置信度；

$G_f(\cdot)$：一个CNN，输入同上，输出是每个点的特征向量；

$F(\cdot)$：一个MLP，在体积渲染阶段用于求一个位置的特征向量，输入是一个点的特征向量和这个点到所求位置的相对位移，输出是该点对这个位置的特征向量贡献. 这个位置附近的所有$K$个点会把它们的特征向量贡献按照上文所说的方法加权平均求得这个位置的特征向量.；

$R(\cdot)$：一个MLP，输入是一个位置的特征向量，输出是这个位置的色彩；

$T(\cdot)$：一个MLP，输入是一个点对一个位置的特征向量贡献，输出是该点对该位置的密度贡献. 

## 疑问

- 显然每张不同的图像都能通过$G_f(\cdot)$生成一组不同的特征向量，而空间中同一个点可能同时对应数个不同的特征向量，本文并没有说明这些特征向量之间是如何取舍的；

- 本文中提到$F(\cdot)$的输入除了一个点的特征向量还包括这个点到所求位置的位移，并特别指出这是为了保持各个点对该位置的特征向量贡献的平移不变性，我没有理解这样的设计是如何保证这一点的；

- 色彩网络$R(\cdot)$作用在特征向量加权求和后，而密度网络$T(\cdot)$作用在特征向量加权求和前. NeRF的做法是将色彩和密度同时放在同一个网络中估计，本文没有解释将它们拆分开来的原因. 

# FENeRF: Face Editing in Neural Radiance Fields

## 研究成果

一个神经网络，以大量人脸图像和它们的语义标签作为数据集进行训练，可以生成一系列以假乱真的人脸图像. 在此基础上，本文实现了几何与材质解耦的图像变形，通过单幅人脸图像生成该人脸在不同视角下的新图像，支持对人脸图像进行局部修改. 

## 核心思想

本文的核心思想是比较好理解的，就是GAN和NeRF的结合，并且在仅与位置有关的密度场和与位置和视角都有关的色彩场之外额外训练了一个语义场，在此基础上本文提出的一个比较新颖的概念是位置特征嵌入，它是一个可学习的向量，用于高频色彩信息的保存. 

需要强调的是，这篇文章并不能直接通过输入一张已有的图像来得到它的各个场的信息，而必须在训练得到的潜在空间中通过梯度下降的方式接近这张图像. 

## 网络架构

![FENeRF Structure](/Assets/Images/FENeRF%20structure.png)

语义特征$Z_s$和材质特征$Z_t$分别是程序按照标准正态分布生成的随机向量，用于生成随机人脸以与判别器对抗，在学习的过程中，这些向量会与数据集中的人脸一起逐渐构成一个人脸的高维潜在空间. 这两种向量各自经过一个高维映射，转换为拟合语义场$S(\mathbf{x})$、密度场$\sigma(\mathbf{x})$和色彩场$C(\mathbf{x},\mathbf{e}_{coord},\mathbf{d})$的一部分的神经网络的权重与拟合色彩场$C(\mathbf{x},\mathbf{e}_{coord},\mathbf{d})$后一部分的神经网络的权重. 

位置特征嵌入$\mathbf{e}_{coord}$和视角$\mathbf{d}$在位置$\mathbf{x}$经过神经网络的第一部分变换后被插入中间向量中. $\mathbf{e}_{coord}$会在神经网络的训练中自动更新，类似于自动解码器的原理. 

在后向传播的过程中，得到更新的参数有：

- 将随机生成的特征向量$Z_s$和$Z_t$映射为神经网络权重的网络参数；

- 语义判别器$D_s$和材质判别器$D_t$的参数；

- 位置特征嵌入$\mathbf{e}_{coord}$. 

网络的损失函数就是经典的GAN损失函数：判别器的判别正确性高则后向传播给判别器的损失就低而后向传播给生成器的损失就高，反之亦然. 值得一提的是，本文的判别器增加了判断视角的功能，并且会与用于生成的视角$d$进行对比，作为损失的一部分仅后向传播给生成器，用于使得生成器能够正确处理视角. 

## 应用

- 将任意两张生成的图像分别对语义特征和材质特征进行插值以单独改变图像的几何信息或材质信息：

    ![FENeRF](/Assets/Images/FENeRF%20Application%201.png)
    
- 通过在潜在空间中梯度下降并接近目标特征向量的方式，将二维人脸图像反向投影到三维空间：

    ![FENeRF](/Assets/Images/FENeRF%20Application%202.png)

- 单独在语义特征的潜在空间中梯度下降，局部修改人像：

    ![FENeRF](/Assets/Images/FENeRF%20Application%203.png)

## 思考

位置特征嵌入$\mathbf{e}_{coord}$对图像生成效果的影响十分显著，但是它嵌入的位置以及作用于整个网络的方式仅仅是通过实验的方式得到了验证，这个小技巧目前仍然缺乏可解释性，本文中也没有对它的含义做出详细的解释. 

# NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction

## 研究成果

改进了NeRF的工作，为了解决更大尺度的场景的重建问题，在NeRF的工作中加入了循环神经网络以融合多个不同空间位置下的场景，同时取得了更快和更大两个成果. 同时在密度场和色彩场的基础上额外学习一个特征场，使得网络能够维护更多几何信息，在细节上的效果也比NeRF更加出色. 

## 网络架构

![NeRFusion Structure](/Assets/Images/NeRFusion%20Structure.png)

这个网络的大部分结构和NeRF是一致的，除了以下几点：

- 在网络的最开始，首先用一个预先训练的CNN提取每张二维图像的特征，并将每个视角下的三维坐标投影到二维平面上，用二维图像中对应点的特征与经过一个MLP作用的视角组合生成这个三维坐标的特征；

- 对于每张二维图像，确定一个包围盒包含所有它视角内的三维坐标，获取与它的视角最接近的$K-1$个视角下的图像对应的三维体积特征，并用这$K$组特征去更新包围盒内的特征. 新的特征由两部分组成，第一部分是这些特征在该点的均值，第二部分是这些特征在该点的方差；

- 采用循环神经网络来融合每张二维图像构成的体积特征，这里采用的是一个比较标准的GRU：

    ![NeRFusion GRU](/Assets/Images/NeRFusion%20GRU.png)

- 如果一个点在每个包围盒中的密度都很低，则删去这个点的特征以提高效率. 

# GeoNeRF: Generalizing NeRF with Geometry Priors

## 研究成果

一个分成两部分的神经网络，前半部分是创新的几何推理器，后半部分是沿袭NeRF的渲染器. 几何推理器能够将学习到的特征在不同场景中迁移，解决NeRF每个场景都要单独训练的问题. 和MVSNeRF相比，GeoNeRF学习场景本身的几何特征，因而能解决遮挡问题. 

## 网络架构

本文的网络架构相当复杂：

![GeoNeRF Structure](/Assets/Images/GeoNeRF%20Structure.png)

基本上就是网络接网络，大部分内容基本就是在原有MVSNeRF架构上的优化，比如用FPN代替用于提取特征的CNN、用AE接MLP代替用于估计空间密度的单独的MLP. 比较值得一提的是其中还有MHA这样的Transformer网络，不过鉴于本人对Transformer一窍不通，实在是无法理解这个网络在这里的作用……

## 疑问

<del>基本全是疑问……</del>

- 我不太确定作为一个多视角学习的网络，这篇文章是如何将输入图像的视角加入网络的输入的，如果我没猜错的话应该是在生成图元（Token）这一步，根据视角生成对应的线性变换$\text{LT}$. 

- GeoNeRF的训练分为可泛化的部分网络的训练和应用网络时理解单个场景的训练，但是我没能成功理解网络的哪些部分是作为可泛化的部分训练完毕的，哪些部分是用于理解单个场景时训练的. 我的猜想是图元（Token）的生成是用于理解单个场景的，但是依然不能理解其中的原理. 

- 本身对Transformer几乎一无所知，这是我自己的问题. 

# CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields

## 研究成果

一个神经网络，给出一张源图像，可以通过其他图像或者词语在这个图像上进行修改，以达成形状或者外观的混合. 

## 网络架构

![CLIP-NeRF Structure](/Assets/Images/CLIP-NeRF%20Structure.png)

- 学习3D占据场/色彩场的部分与根据这两个场生成图像的部分和NeRF基本一致，但输入除了标准的三维坐标和相机角度以外还有场景的形状编码$z_s$与外观编码$z_a$；

- 利用CLIP进行词语和图像的混合编码，分别通过一个MLP得到形状编码与外观编码的修正量$\Delta z_s$和$\Delta z_a$；

- 三维坐标也像NeRF一样进行了三角函数分解以保留高频特征；

- 将三维坐标和修正后的形状编码输入一个形变场MLP中，将得到的新编码连接在三维坐标后；

- 形状编码$z_s$、外观编码$z_a$和相机角度$v$最初是由数据集给出用来训练网络的，在测试的过程中，输入图像的编码则是由搜索的方式在编码构成的流形上插值得出的，而相机角度则是通过最小化损失函数得出；

- CLIP编码器是预先训练的；

- 用一个判别器$\mathcal{D}$训练生成图像的真实度；

- 再次运用预先编码的CLIP比较目标图像/词语与生成图像的编码的距离作为训练损失的一部分. 

关于训练流程：

- 这个网络的训练并不是端到端的，首先在零形状编码与外观编码修正量的情况下单独用一个判别器来对抗学习密度场和色彩场的MLP和学习形变场的MLP；

- 其次固定其他网络模块的权重，训练将CLIP的输出转化为两个编码的修正量的MLP. 

## 疑问

- 本文声称不采用形变场会导致$\Delta z_s$同时影响生成图像的色彩，但是为什么加了这个东西就能避免这个情况呢？训练的过程依然是没有改变的. 

